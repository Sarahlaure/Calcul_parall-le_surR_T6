---
title: "EXPOSE CALCUL PARALLELE"
author: "Fogwoung Djoufack Sarah-Laure & Aissatou Sega Diallo"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
fontsize: 12pt
---

# INTRODUCTION

## Contexte et motivations ayant donné lieu au calcul parallele

L’ère du Big Data, marquée par l’explosion des réseaux sociaux, de l’IoT (Internet des Objets), et des simulations numériques, a entraîné une croissance vertigineuse des données. Selon IBM, 90 % des données mondiales ont été générées au cours des deux dernières années seulement – un rythme qui s’accélère avec plus de 2,5 trillions d’octets créés quotidiennement (https://bernardmarr.com/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/.

Les applications critiques comme l’analyse financière en temps réel, l’entraînement de modèles de Deep Learning (ex : GPT-4 avec 1,7 trillion de paramètres), ou les prévisions météorologiques à haute résolution, exigent désormais des traitements en millisecondes. Or, un processeur monocœur classique ne peut traiter qu’environ 3 milliards d’instructions par seconde (3 GHz), rendant le calcul séquentiel totalement inadapté à ces défis.

### Loi de Moore et plafonnement des fréquences CPU 

Les processeurs modernes reposent sur des milliards de transistors, des micro-interrupteurs qui exécutent des calculs. Selon la loi de Moore (1965), leur nombre double tous les deux ans, permettant pendant des décennies d’augmenter la fréquence d’horloge (GHz), c’est-à-dire le rythme auquel ces transistors s’activent. Ainsi, un CPU à 3 GHz réalise 3 milliards d’opérations par seconde. Cependant, au-delà de ~4 GHz, la chaleur et la consommation d’énergie deviennent ingérables. 

Pour contourner cette limite, l’industrie a abandonné l’idée d’un cœur unique ultra-rapide et a opté pour le multi-core en ajoutant plusieurs cœurs sur une même puce.Chacun fonctionne à une fréquence raisonnable, mais leur travail combiné permet de traiter des tâches en parallèle. Surtout, le multi-core répartit la chaleur sur toute la surface du CPU, évitant les points chauds et facilitant le travail des systèmes de refroidissement (ventilateurs, radiateurs). Cette approche est aujourd’hui indispensable pour le Big Data ou l’apprentissage automatique, où des milliards de données doivent être traitées sans surchauffer les machines.

## Présentation du calcul parallele 

### Définition 

Le **calcul parallèle** désigne l'exécution simultanée de plusieurs tâches grâce à l'utilisation de plusieurs ressources informatiques (comme des cœurs ou des threads, dont nous parlerons ci-dessous). Plutôt que d’exécuter les opérations une par une (calcul séquentiel), le calcul parallèle divise un problème en sous-problèmes plus petits qui peuvent être résolus en même temps. Cette approche permet de réduire significativement le temps de traitement des tâches, surtout lorsque le problème est volumineux ou complexe.

`![Texte alternatif](../IMAGE RMD/Principe seq vs parallele.png)

### Comparaison calcul parallele vs calcul sequentielle 

`![Texte alternatif](../IMAGE RMD/Comparaison seq vs parallele.png)

# Concepts fondamentaux 

Le processeur (ou CPU, pour Central Processing Unit) est la partie qui exécute les instructions des programmes et effectue les calculs nécessaires au bon fonctionnement de l'ordinateur. Il peut être vu comme "l'organe" qui contrôle l'exécution des tâches. Un processeur peut contenir un ou plusieurs cœurs (ou cores), chacun capable de traiter des instructions indépendamment.

## 1. Coeurs du Processeur

**Définition :**  
Un **cœur** (ou core) est une unité de calcul indépendante au sein d’un même processeur. Chaque cœur dispose de son propre jeu de registres, d’une unité arithmétique/logique et d’un cache. Plus un processeur compte de cœurs, plus il peut traiter de tâches vraiment en parallèle. 

- **Exemple :**  
  Un processeur *quad-core* possède 4 coeurs physiques, chacun pouvant exécuter des instructions indépendamment des autres.

- Par ailleurs, il ne faut pas se fier aux noms marketing: un Intel Core i5 n’a pas nécessairement 5 coeurs ! Par exemple, un i5-10400 possède 6 coeurs physiques. Les nombres dans les noms (i3, i5, i7) reflètent une gamme de performance, pas le nombre de coeurs.

- Pour connaître le nombre de coeurs disponibles sur votre machine, vous pouvez utiliser la fonction `detectCores()` du package **parallel**, avec l'option FALSE sur logical. 

```{r coeurs, message=FALSE, warning=FALSE }
# Ce package n'a pas besoin d'être téléchargé au préalable,
# il est directement disponible lorsque R est installé 
library(parallel) 

# Nombre de coeurs physiques
nb_coeurs_physiques <- detectCores(logical = FALSE)
print(nb_coeurs_physiques)
``` 

Dans mon cas par exemple, mon core i5 a 10 coeurs.


## 2. Threads (ou Processeurs Logiques)

- **Définition :**  
Un **thread** est la plus petite unité d’exécution gérée par le système d’exploitation, c’est une séquence d’instructions qu’un cœur peut traiter.

Chaque thread possède son propre contexte (ensemble des informations que le CPU doit sauvegarder et restaurer pour reprendre l’exécution exactement là où il s’était arrêté), mais partage avec les autres threads du même cœur les ressources matérielles (unités arithmétiques, caches).

- Pour connaître le nombre de threads disponibles sur votre machine, vous pouvez utiliser la même fonction `detectCores()` du package **parallel**, avec l'option TRUE sur logical. 

```{r threads, message=FALSE, warning=FALSE}
# Nombre de threads (unités logiques)
nb_threads <- detectCores(logical = TRUE)
print(nb_threads)

``` 

Ces deux informations peuvent aussi etre visualisées au niveau du gestionnaire de tâches, sous l'onglet performance.

`![Texte alternatif](../IMAGE RMD/Coeurs_Threads.png)

## 3.**Hyper-threading :**  
  C'est une technologie qui permet à un seul cœur physique de se présenter au système d'exploitation comme deux (et quelque fois plus de 2) cœurs logiques (threads) simultanés.  
  
  - **Exemple :**  
    Un processeur quad-core sans hyper-threading gère 4 threads mais lorsqu'il est  doté d'hyper-threading, il peut gérer 8 threads. De manière generale le système d'exploitation considere donc que l'ordinateur a 8 coeurs puisqu'il peut théoriquement exécuter 8 tâches en même temps, mais en réalité, c'est rien que 4 coeurs physiques qui existent réellement. 

### Lien entre Coeurs, Threads et calcul parallele

- **Physiquement :**  
  Vous disposez d'un nombre fixe de coeurs physiques (par exemple, 4 coeurs sur un processeur quad-core).
  
- **Logiquement :**  
  Chaque coeur peut être "dédoublé" en plusieurs threads grâce à l'hyper-threading. Ainsi, même avec 4 coeurs, vous pouvez avoir plus d'unités d'exécution simultanée (par exemple, 8 threads).

- **En pratique pour le calcul parallèle :**  
  On parle souvent de "coeurs" pour simplifier, mais ce sont en réalité les **threads** (unités logiques) qui exécutent les tâches. Le nombre de threads disponibles détermine combien de tâches peuvent être réellement exécutées en parallèle.

Par exemple, dans mon cas, mon ordinateur a 10 coeurs et 12 threads et donc il y a 2 coeurs qui grace au hyper-threading gerent 2 threads chacun et les 8 autres coeurs gerent 1 thread chacun. Donc pour le calcul parallele, 12 threads sont disponibles pour optimiser cette méthode.

### 4. Programme Maître et son rôle dans le calcul parallèle

Le programme maître est essentiel pour coordonner le calcul parallèle en distribuant les tâches et en recueillant les résultats. Il est chargé de trois responsabilités principales :

1. **Initialisation des workers**  

   - Création des workers et chargement des packages nécessaires. 
   
   - **Coût d’initialisation** : cette étape peut prendre plusieurs secondes, car chaque worker doit démarrer et recevoir les outils nécessaires.  
    
     - **Amortissement de l'overhead** : pour que le parallélisme soit rentable, chaque worker doit avoir **suffisamment de travail** pour compenser ce temps de démarrage, dans le cas contraire, le calcul séquentiel peut etre preferable. 
     
    - *Exemple*: Si le démarrage d’un worker prend 100 ms et que sa tâche dure 10 ms, le coût d’overhead est prohibitif. 
( L’**overhead** ou **surcharge** regroupe toutes les opérations **hors calcul** qui consomment du temps:
- Création et destruction des workers  
- Exportation des données et des fonctions  
- Chargement des packages  
- Communication entre le maître et les workers  
Ces coûts doivent être **compensés** par la durée des calculs parallèles pour que l’approche soit bénéfique.)

2. **Répartition des tâches:**  

Le programme maître divise une tâche globale en plusieurs sous-tâches qui peuvent être exécutées simultanément par différents threads ou cœurs de processeur. L'objectif est de distribuer la charge de travail pour maximiser l'efficacité de l'exécution parallèle.
Ainsi, chaque thread travaille sur une portion de la tâche, ce qui accélère l'exécution du programme.

3. **Lancement en parallèle:**  

Une fois que les sous-tâches sont réparties, le programme maître envoie chaque sous-tâche aux threads disponibles pour exécution simultanée. Ces threads peuvent être répartis sur les différents cœurs de processeur ou machines.

- **Collecte et agrégation des résultats:**  
Après que chaque thread a terminé sa partie du travail, le programme maître récupère les résultats produits. 

D'autres part, nous distinguons les sous-programmes (ou workers).

### 5. Cluster et worker 

Un **cluster** est l’ensemble des processus R (ou threads) que l’on lance pour exécuter un calcul en parallèle.  
- On crée un cluster avec `makeCluster(n)`, où `n` est le nombre de workers souhaités.  
- Le cluster sert à **répartir** le travail sur plusieurs unités de calcul (cœurs ou machines).

Un **worker** est un **processus individuel** du cluster :  
- Il reçoit **une sous‑tâche** du programme maître.  
- Il exécute la fonction assignée sur son lot de données.
- Il renvoie son résultat au programme maître pour agrégation.

D'autre part, on distingue le **cluster** est un ensemble de processus R (ou de threads) lancés pour exécuter des tâches en parallèle.  
- Chaque processus du cluster est appelé un **worker**.  
- Le cluster permet de répartir le travail sur plusieurs unités de calcul (cœurs ou machines).

**Remarque**  
- On peut créer plus de workers que de threads logiques.
- Dans ce cas, les workers excédentaires restent en **attente** jusqu’à ce qu’un thread se libère.  
- Il faut veiller à ce que chaque worker ait **suffisamment** de travail pour amortir le coût de démarrage du cluster. 


-- Mise en oeuvre avec R (structure globale)

Les fonctions proviennent du package parallele aussi. 

Etape 1: on initialise un cluster, c’est-à-dire un groupe de workers, en précisant combien on en veut (par exemple, dans notre cas, on definit autant que le nombre de threads disponibles: **cl <- makeCluster(nb_threads)**).

Etape 2: Puis la fonction parLapply() permet de distribuer les sous-tâches à chaque worker. On lui donne une liste tâches  et une fonction f à appliquer à chaque élément. (**résultats <- parLapply(cl, tâches, f)**). 

Etape 3: Arrêt du cluster (libération des ressources): Une fois toutes les tâches terminées, on arrête proprement les workers.Cela libère la mémoire et évite de laisser des processus inutilisés tourner en arrière-plan. On fait de ce fait: **stopCluster**. 

Etape 4: Chaque worker renvoie un résultat (souvent un tableau ou un vecteur).On utilise une fonction pour assembler tous les résultats obtenus en un seul tableau cohérent, ligne par ligne.

# Packages et fonctions de base du calcul parallele

## 1. Package **parallel**

Ce package fait partie de l’installation standard de R et est utilisé pour répartir des tâches entre les cœurs d’un même ordinateur (et donc dans un parallélisme partagé). 

- `makeCluster(n)` : crée un cluster de `n` workers.
- `clusterExport(cl, varlist)` : exporte les objets R nécessaires aux workers dans leurs environnements respectifs.
- `clusterEvaLQ(cl, expr): 	Exécute une commande dans tous les workers
- `parLapply(cl, X, FUN)` : version parallèle de `lapply. La base de données ou tout autre objet X est divisé en sous ensemble qui seront traité par un worker différent du cluster cl et chaque worker applique la fonction FUN a chaque élément à sa charge. Et à la fin les résultats sont présentés sous forme de liste. 
- `parSapply(cl, X, FUN)` : Meme principe que parLapply mais ici le résultat est simplifié et n'est pas forcément sous forme d'une liste. 
- `mclapply(X, FUN, mc.cores)` : version parallèle de `lapply` pour Unix/Linux/Mac (pas Windows).
- `stopCluster(cl)` : arrête le cluster.
- `clusterApply()`  : applique la fonction fun sur les éléments de x, chaque élément étant envoyé à un worker du cluster cl.

- `clusterMap(cl, fun, ...)`: applique fun de manière parallèle sur plusieurs listes d’arguments (comme mapply).

## 2. Package **foreach + doParallel** 

Ces packages permettent de créer des boucles parallèles. foreach fournit la syntaxe, tandis que doParallel permet de connecter cette syntaxe à un backend de calcul parallèle (comme un cluster).er.

Ses principales fonctions sont: 
- `registerDoParallel(n)`: crée un cluster de `n` workers pour exécuter des boucles en parallèle.
- `foreach()`: Permet d'itérer de manière parallèle sur une séquence donnée.
- `%dopar%`: L'opérateur utilisé avec foreach pour paralléliser l'exécution de la boucle.

```{r doParallel, message=FALSE, warning=FALSE}
library(doParallel)
cl <- makeCluster(4)
registerDoParallel(cl)
result <- foreach(i = 1:100) %dopar% sqrt(i)
stopCluster(cl)

print(resultats)
``` 
Ici, le calcul de racines carrées est réparti entre 4 cœurs. Le cœur qui termine le plus vite se voit attribuer la prochaine tâche disponible. 


## 3. Packages **future** et **future.apply**

Les packages `future` et `future.apply` ont été conçus pour rendre le calcul parallèle en R à la fois plus simple, plus lisible et plus flexible. Contrairement à des packages comme `parallel`, où il faut explicitement créer un cluster, gérer les workers et exporter les variables, ici on adopte une approche déclarative : **on décrit ce qu’on veut faire**, et **le système se charge de l’exécuter** selon un plan défini.

Ces packages permettent de paralléliser facilement des fonctions comme `lapply()` ou `sapply()` **sans avoir à se soucier des aspects techniques** (gestion des cœurs, communication entre workers, etc.). Il devient ainsi très simple de basculer d'une exécution séquentielle à une exécution parallèle, en changeant une seule ligne de code.

Ses principales fonctions sont: 

- `plan(strategy)`  
  Cette fonction définit le **plan d’exécution**, c’est-à-dire la manière dont les tâches vont être exécutées :
  - `plan(sequential)` : exécution classique, une tâche après l'autre (par défaut).
  - `plan(multisession)` : exécution parallèle sur plusieurs cœurs, chaque tâche étant lancée dans un processus indépendant. Fonctionne sur **tous les systèmes d’exploitation**, y compris **Windows**.
  - `plan(multicore)` : exécution parallèle optimisée pour **Linux/Mac** uniquement, en utilisant des processus légers (forks).

- `future_lapply(X, FUN)`  
  Version parallèle de `lapply()`. Chaque élément de `X` est traité indépendamment par la fonction `FUN`, de manière simultanée sur les différents cœurs disponibles. Le résultat est retourné sous forme de **liste**.

- `future_sapply(X, FUN)`  
  Version parallèle de `sapply()`. Fonctionne comme `future_lapply()` mais **le résultat est automatiquement simplifié** (vecteur, matrice...), comme avec `sapply()`.

```{r doParallel, message=FALSE, warning=FALSE}
library(future)
library(future.apply)

# Définir un plan d’exécution parallèle
plan(multisession)

# Exécuter une opération en parallèle
resultats <- future_lapply(1:10, function(x) sqrt(x))

# Afficher les résultats
print(resultats)
```

## 4. Package **snow** (Simple Network of Workstations)

**snow** est une alternative plus flexible à parallel, notamment pour les environnements distribués (plusieurs machines), mais fonctionne aussi en local. 

Il comporte les fonctions suivantes: 

- `makeCluster()`: crée un cluster de workers, localement ou à distance.

- `clusterExport()`: clusterApply(), clusterMap(): mêmes principes que dans parallel.

- `snow.time()`: mesure le temps d'exécution d’une commande dans un cluster.


## 5. Package **snowFT** 
**snowFT** est une extension du package snow permettant la tolérance aux pannes (fault tolerance). Il est utile dans les environnements instables ou avec de longs calculs, car il permet de redémarrer un worker qui a échoué sans compromettre tout le processus.

Ses principales fonctions sont: 

- `makeClusterFT()`: crée un cluster avec tolérance aux pannes.
Compatible avec les fonctions classiques de snow.


## 6. Package **BiocParallel** 
éveloppé pour la communauté Bioconductor, ce package est particulièrement adapté aux traitements de données biologiques, mais peut être utilisé plus largement. 

Ses principales fonctions sont: 
- `bplapply(X, FUN, BPPARAM)`: version parallèle de lapply utilisant le backend précisé dans BPPARAM.

- `bpstart(), bpstop()`: pour démarrer et arrêter un backend.

- `bpvec(), bpaggregate()`: fonctions pour les grands vecteurs ou les regroupements parallèles.

## 7. Package **sparklyr** 
sparklyr permet de connecter R à Apache Spark, une plateforme de calcul distribué à grande échelle. Il est utilisé pour traiter de très gros volumes de données en parallèle sur des clusters Spark.

Fonctions principales :

- `spark_connect()`: connecte R à un cluster Spark.

- `sdf_copy_to()`: copie un data.frame R vers Spark.

- `spark_apply(df, function): applique une fonction R en parallèle sur les partitions de données Spark. 

## 8. Package **tictoc** 

Ce package mesure le temps que prend une tâche à s'exécuter.
Ses fonctions clé sont: 
- `tic(label)`: Pour lancer le chronomètre , le label étant optionnel et servant à identifier le chronometre.   
- `toc`: Arrête le chronometre et affiche le temps écoulé 


##	Les différents types de parallélisme 

Dans cette partie, nous explorons les types de parallélisme que l'on peut utiliser pour diviser une tâche complexe en plusieurs sous-tâches exécutées simultanément.

### Parallélisme des données vs paralléliste des taches 

#### Parallélisme de données

Le **parallélisme de données** consiste à prendre un même traitement et à l’appliquer **simultanément** à plusieurs **sous‑ensembles** d’un grand jeu de données.  
- **Principe** : on divise les données en morceaux indépendants (blocs de lignes, groupes de pays, plages horaires).  
- **Exécution** : chaque cœur (ou worker) reçoit un sous‑ensemble et réalise exactement la même opération (tri, somme, moyenne, etc.).  
- **Quand l’utiliser** :  
  - Traitement de **grandes bases de données** (Big Data)
  - Opérations répétitives et uniformes (ex. calcul de moyennes, transformations de colonnes)
  
- **Avantage** : excellente montée en charge dès que le volume de données est important, car chaque cœur peut travailler de son côté sans attendre les autres.

 **Application ** : Le programme maitre fait la preparation globale des données (chargement, filtre et identification des types uniques), puis initie le cluster (en definissant donc le nombre de workers qui seront au nombre de 10 ici) puis distribute les données à chaque worker.
Durant cette phase, chaque worker exécute séquentiellement calculer_moyenne() sur ses types assignés, sans interférence avec les autres. Cette isolation garantit l’absence de conflits de mémoire mais nécessite une duplication initiale des données.
Une fois tous les résultats partiels reçus, le maître utilise bind_rows() pour fusionner les dataframes individuels en un seul tableau. Enfin, stopCluster() termine proprement les workers, libérant les ressources système. 
 
```{r parallele , warning=FALSE, message=FALSE}

ships <- read_csv("../data/ships.csv")  
# 1. Préparation des données
donnees_filtrees <- ships %>%
  filter(!is.na(SPEED), SPEED > 0, !is.na(ship_type))

types_navires <- unique(donnees_filtrees$ship_type)

# Fonction de calcul de la moyenne pour un type de navire
calculer_moyenne <- function(type) {
  donnees_filtrees %>%
    filter(ship_type == type) %>%
    summarise(type_navire = type,
              vitesse_moyenne = mean(SPEED, na.rm = TRUE))
}

# 2. Fonction de calcul parallèle avec 8 workers
parallele <- function() {
  # Créer le cluster avec 8 workers
  cl <- makeCluster(8)

  # Exporter les objets nécessaires aux workers
  clusterExport(cl, varlist = c("donnees_filtrees", "types_navires", "calculer_moyenne"), envir = globalenv())

  # Charger dplyr dans chaque worker
  clusterEvalQ(cl, library(dplyr))

  # Mesurer le temps de calcul
  temps_calc <- system.time({
    res <- parLapply(cl, types_navires, calculer_moyenne)
  })[3]

  # Arrêter le cluster
  stopCluster(cl)

  # Agréger les résultats
  resultat_final <- bind_rows(res)

  return(list(resultats = resultat_final, temps = temps_calc))
}

# 3. Appel de la fonction et affichage des résultats
resultats <- parallele()
cat("Temps de calcul avec 8 workers :", resultats$temps, "secondes\n")

```


### Parallélisme de tâches

Le **parallélisme de tâches** consiste à exécuter **différentes opérations** en même temps, sur un même ou plusieurs jeux de données.  
- **Principe** : on définit plusieurs tâches **distinctes** (nettoyage, calcul de statistiques, visualisation, export), puis on les lance **en parallèle**.  
- **Exécution** : chaque worker prend en charge une tâche spécifique, sans interférer avec les autres.  
- **Quand l’utiliser** :  
  - Chaînes de traitement où chaque étape est indépendante  
  - Workflows complexes (prétraitement, analyse, génération de rapports)  
- **Avantage** : accélère l’exécution globale d’un pipeline en répartissant des opérations hétérogènes sur plusieurs ressources.

**Exemple*:
Une tâche fait le nettoyage des données, une autre effectue une analyse statistique, et une autre génère des graphiques. Ces trois tâches sont effectuées en parallèle.

**Application:** Exécuter en parallèle trois traitements différents sur les données de navires : 1 worker pour calculer la vitesse moyenne par type de navire, 1 navire pour effectuer une regression linéaire entre la vitesse et la longueur des navires, un autre qui génre un rapport statistique avec médiane et écart-type.

```{r parallelisme_taches , warning=FALSE, message=FALSE}
# Étape 1 : Préparation des données 
donnees_filtrees <- ships %>%
  filter(!is.na(SPEED), SPEED > 0, !is.na(ship_type))

# Étape 2 : Définition des tâches hétérogènes (chaque tâche est une fonction différente)
tache_moyennes <- function(data) {
  data %>%
    group_by(ship_type) %>%
    summarise(vitesse_moyenne = mean(SPEED, na.rm = TRUE))
}

tache_regression <- function(data) {
  lm(SPEED ~ LENGTH, data = data)  # Modèle linéaire
}

tache_statistiques <- function(data) {
  data %>%
    group_by(ship_type) %>%
    summarise(
      mediane = median(SPEED, na.rm = TRUE),
      ecart_type = sd(SPEED, na.rm = TRUE)
    )
}

# Étape 3 : Configuration du cluster pour 3 workers (un par tâche)
cl <- makeCluster(3)

# Exportation des données et packages vers chaque worker (une seule fois)
clusterExport(cl, c("donnees_filtrees", "tache_moyennes", "tache_regression", "tache_statistiques"))
clusterEvalQ(cl, library(dplyr))

# Étape 4 : Attribution des tâches aux workers avec chronométrage (tictoc)
tic("Exécution des tâches en parallèle")  # Démarrer le chronomètre global
resultats <- parLapply(cl, list(tache_moyennes, tache_regression, tache_statistiques), function(f) {
  f(donnees_filtrees)  # Chaque worker exécute une fonction différente
})
toc()  

# Étape 5 : Arrêt du cluster et extraction des résultats
stopCluster(cl)

# Extraire les résultats
resultats_moyennes <- resultats[[1]]  # Vitesses moyennes
resultats_regression <- resultats[[2]]    # Objet de régression
resultats_stats <- resultats[[3]]     # Statistiques

# Fusionner les résultats dans un seul tableau
tableau_resultats <- resultats_moyennes %>%
  left_join(resultats_stats, by = "ship_type") %>%
  mutate(coef_intercept = resultats_regression[1],  # Ajouter le coefficient d'interception de la régression
         coef_slope = resultats_regression[2])  # Ajouter le coefficient de pente de la régression

# Affichage du tableau final
cat("Tableau récapitulatif des résultats :\n")
print(tableau_resultats)

```

Pour le parallelisme de taches, il est généralement conseillé de **fixer le nombre de workers au plus égale nombre de cœurs physiques** de votre machine:

En effet, dans le cas où on a **Plus de workers que de cœurs**, même si l'hyper‑threading permet à un cœur de traiter plusieurs tâches simultanément, les threads d'un même cœur **partagent les mêmes ressources**.  
  Ainsi, lorsque plusieurs tâches lourdes sont exécutées en même temps et qu’un même cœur doit se les partager, il doit diviser ses ressources (comme le temps de calcul) entre elles. Cela crée une forme d’encombrement, ce qu’on appelle une congestion, qui ralentit l’exécution des tâches à cause de la compétition entre les workers pour accéder aux ressources, et de la surcharge liée à leur gestion.

En parallélisme de données, le nombre de workers doit être proche du nombre de cœurs physiques pour les calculs lourds. Si les tâches sont rapides, on peut augmenter les workers, mais de maniere prudente. 

## Parallélisme distribué et parallélisme partagé 

Il existe deux principales approches pour utiliser plusieurs processeurs afin de faire du calcul parallèle : le parallélisme distribué et le parallélisme partagé (ou calcul parallele en local). Ces approches déterminent comment les ressources sont partagées entre plusieurs processeurs, qu'ils soient dans un même ordinateur ou répartis sur plusieurs machines.


Le *parallélisme partagé* est basé sur le fait que tous les workers accèdent à la même mémoire centrale RAM.
C'est le modèle utilisé **lorsqu'on travaille sur une seule machine**, équipée de plusieurs cœurs de processeur.

D'autre part, le *parallélisme distribué* va plus loin en répartissant le calcul sur plusieurs machines connectées par un réseau. Chaque machine travaille sur une portion des données et communique avec les autres machines pour échanger des informations ou combiner les résultats. Dans ce cas, chaque machine peut avoir sa propre mémoire et ses propres ressources, et elles doivent s'échanger des données via un réseau (comme Internet ou un réseau local).

## Patterns essentiels 

### 1. Fork-join

**Principe :**  
On découpe une tâche en plusieurs sous‑tâches indépendantes (fork), on exécute ces sous‑tâches simultanément sur plusieurs cœurs, puis on combine (join) leurs résultats pour produire la réponse finale.

**Étapes :**  
1. **Fork** : création de sous‑tâches à partir de la tâche principale.  
2. **Exécution parallèle** : chaque sous‑tâche s’exécute sur un cœur distinct.  
3. **Join** : agrégation des résultats partiels pour reconstituer la solution complète.  

`![Texte alternatif](../IMAGE RMD/fork join.png)

```{r fork-join, echo=TRUE, message=FALSE, warning=FALSE}
# Tâche principale : additionner quatre vecteurs de 1e6 nombres
vec_list <- list(rnorm(1e6), rnorm(1e6), rnorm(1e6), rnorm(1e6))

# Fork-Join
n_cores <- 4
cl <- makeCluster(n_cores)

# Fork : on distribue chaque vecteur à un cœur
results <- parLapply(cl, vec_list, function(v) sum(v))

# Join : on additionne les sommes partielles
total_sum <- Reduce("+", results)

stopCluster(cl)
total_sum
```

### 2. Divide & Conquer

**Principe :**  
On résout un problème en le divisant **récursivement** en sous‑problèmes plus petits, jusqu’à ce qu’ils soient assez simples pour être résolus directement. Ensuite, on **combine** les solutions de ces sous‑problèmes pour obtenir la solution du problème initial.

**Étapes :**  
1. **Diviser** : séparer le problème en deux (ou plusieurs) sous‑problèmes de taille réduite.  
   - *Aucun tri n’est effectué à cette étape*, on ne fait que découper la liste.  
2. **Conquer (Régner)** : résoudre chaque sous‑problème, éventuellement en réappliquant récursivement la même méthode, jusqu’à obtenir des sous‑listes de taille 1 (déjà triées).  
3. **Combiner** : fusionner les solutions des sous‑problèmes pour former la solution finale.  

`![Texte alternatif](../IMAGE RMD/Divide_conquer.jpeg)

### 3. Map Reduce

Le modèle MapReduce est utilisé pour traiter de grandes quantités de données en parallèle, c'est-à-dire en les traitant simultanément, mais de manière organisée. Il consiste en 4 étapes principales : Division des données, Map, Shuffle (Regroupement) et Reduce.

`![Texte alternatif](../IMAGE RMD/Map_reduce.png)

**Étapes :**

1. **Division des données ** : Dans la phase de division, les données sont découpées en morceaux plus petits, pour un traitement plus rapide et facile. 

2. **Map ** : Une fois les données découpées, la phase Map entre en jeu. La fonction Map est appliquée à chaque morceau de données. Elle prend un morceau en entrée, le traite et génère des paires clé-valeur.La clé peut être n'importe quel identifiant ou valeur qui représente un élément spécifique du morceau de données. Par ailleurs, la valeur est un nombre qui représente l'information associée à cette clé. Dans beaucoup de cas, cette valeur est simplement un nombre 1, pour signifier que l'élément a été trouvé ou observé une fois. 

3. **Shuffle **: Une fois les paires clé-valeur générées, il est temps de les regrouper pour les préparer à la phase Reduce. Elle consiste à regrouper toutes les paires ayant la même clé ensemble.

4. **Reduce **: La phase Reduce consiste à agréger les données regroupées par la phase Shuffle. Pour chaque groupe de paires ayant la même clé, une fonction Reduce est appliquée pour combiner ou résumer les valeurs.

**Exemple 1 :** Comptage des fréquences de chaque mot dans un texte 

```{r map-reduce1, echo=TRUE, message=FALSE, warning=FALSE}

# Texte à analyser (exemple simple)
phrases <- c(
  "Le chat mange le poisson",
  "Le chien court après le chat",
  "Le poisson nage dans l'eau"
)

# --- Phase MAP ---
map_function <- function(phrase) {
  # Découpage de la phrase en mots (clés)
  mots <- unlist(strsplit(tolower(phrase), "\\W+"))  # Ignorer la casse et la ponctuation
  # Génération des couples (mot, 1)
  return(data.frame(mot = mots, valeur = 1))
}

# --- Phase REDUCE ---
reduce_function <- function(cles, valeurs) {
  # Agrégation : somme des valeurs par clé
  aggregate(valeur ~ mot, data = data.frame(mot = cles, valeur = valeurs), sum)
}

# Configuration du cluster (4 cœurs)
cl <- makeCluster(4)
clusterExport(cl, c("map_function", "reduce_function"))

# --- MAP ---
# Appliquer la fonction map à chaque phrase en parallèle
resultats_map <- parLapply(cl, phrases, map_function)

# --- SHUFFLE (Regroupement des clés) ---
# Combiner tous les résultats partiels
donnees <- do.call(rbind, resultats_map)

# --- REDUCE ---
# Appliquer la fonction reduce sur les données groupées
resultat_final <- reduce_function(donnees$mot, donnees$valeur)

# Arrêt du cluster
stopCluster(cl)

print(resultat_final)

```


# PERFORMANCE ET ASPECTS PRATIQUES 

## Métriques de base 

### Speedup

Le speedup se définit comme le ratio du temps d'exécution d'une tâche en mode séquentiel (T₁) sur le temps en mode parallèle (Tₚ) :
$$
\text{Speedup} = \frac{T_1}{T_p}
$$
--- Speedup pour une tache légère (pour montrer que dans ce cas, le calcul parallele est plus couteux)

Ici on utilise une fonction qui calcule la somme des carrés des entiers de 1 à 10 000 suivant les methodes parallele et sequentielle. 

```{r speedup1, message=FALSE, warning=FALSE}
library(parallel)
library(tictoc)

# Fonction séquentielle
tache_sequentielle <- function(n = 1e4) {
  sum((1:n)^2)
}

# Fonction parallèle
tache_parallele <- function(n = 1e4, coeurs = 2) {
  cl <- makeCluster(coeurs)  # Création d'un cluster de travail avec un nombre spécifié de cœurs
  clusterExport(cl, varlist = "n")  # Exporter la variable 'n' dans les processus du cluster
  resultat <- parLapply(cl, split(1:n, cut(1:n, coeurs)), function(x) sum(x^2))  # Calcul parallèle des carrés, chaque sous-ensemble 'x' est traité sur un cœur
  stopCluster(cl)  # Arrêter le cluster une fois le calcul terminé
  Reduce("+", resultat)  # Agréger les résultats obtenus sur les différents cœurs
}

# Liste des cœurs à tester
liste_coeurs <- c(1, 2, 3, 4, 5, 6, 8)

# Initialisation du tableau des résultats
resultats_simples <- data.frame(Cœurs = liste_coeurs, Temps = NA, Speedup = NA)

# Temps de référence (séquentiel)
temps_seq <- system.time(tache_sequentielle())[3]

# Remplissage du tableau
for (i in seq_along(liste_coeurs)) {
  nb_coeurs <- liste_coeurs[i]
  
  if (nb_coeurs == 1) {
    resultats_simples$Temps[i] <- round(temps_seq, 4)
    resultats_simples$Speedup[i] <- 1
  } else {
    temps_par <- system.time(tache_parallele(1e4, nb_coeurs))[3]
    resultats_simples$Temps[i] <- round(temps_par, 4)
    resultats_simples$Speedup[i] <- round(temps_seq / temps_par, 2)
  }
}

# Affichage du tableau final
resultats_simples

```
En effet, on observe que la version séquentielle de la fonction prend environ 0,32 secondes, tandis que la version parallèle devient de plus en plus lente à mesure qu’on augmente le nombre de cœurs. Le tableau récapitulatif montre ainsi que le speedup est inférieur à 1 dans tous les cas, ce qui signifie qu’il n’y a aucun gain de performance – au contraire, l'exécution parallèle est plus lente. Cela s’explique par le coût de la création du cluster, de la répartition des données, et de la communication entre les cœurs, qui devient significatif face à une tâche aussi légère. Ces résultats montrent clairement que le parallélisme n’est pas avantageux pour les tâches simples. 

--- Speedup pour une tache complexe 

Produit matricielle de matrice de taille 1000*1000

Pour le calcul parallele, on donne des groupes de C_{ij} avec \[
C_{ij} = \sum_{k=1}^{n} A_{ik} \times B_{kj}
\] à différents coeurs pour les calculs. 

```{r speedup2, message=FALSE, warning=FALSE}
library(parallel)
library(tictoc)

# Fonction pour multiplier une sous-matrice (partie d'une matrice)
multiplication_matrice <- function(A, B, lignes, colonnes) {
  res <- matrix(0, nrow = length(lignes), ncol = length(colonnes))
  for (i in seq_along(lignes)) {
    for (j in seq_along(colonnes)) {
      res[i, j] <- sum(A[lignes[i], ] * B[, colonnes[j]])  # Produit scalaire des lignes et colonnes
    }
  }
  return(res)
}

# Fonction principale pour multiplier deux grandes matrices en parallèle
calcul_multiplication_parallele <- function(A, B, sections = 4) {
  n <- nrow(A)
  # Diviser les lignes et colonnes en sous-ensembles équilibrés
  lignes_par_section <- split(1:n, cut(1:n, sections, labels = FALSE))
  colonnes_par_section <- split(1:n, cut(1:n, sections, labels = FALSE))
  
  # Créer un cluster
  cl <- makeCluster(detectCores())  # Utilise tous les cœurs disponibles
  clusterExport(cl, varlist = c("A", "B", "multiplication_matrice"))  # Exporte seulement les variables nécessaires
  
  # Paralléliser la multiplication
  resultats <- parLapply(cl, 1:sections, function(i) {
    lignes <- lignes_par_section[[i]]
    colonnes <- colonnes_par_section[[i]]
    multiplication_matrice(A, B, lignes, colonnes)
  })
  stopCluster(cl)
  
  # Combiner les résultats dans la matrice finale
  matrice_resultat <- matrix(0, nrow = n, ncol = n)
  for (i in 1:sections) {
    lignes <- lignes_par_section[[i]]
    colonnes <- colonnes_par_section[[i]]
    matrice_resultat[lignes, colonnes] <- resultats[[i]]
  }
  
  return(matrice_resultat)
}

# Créer deux grandes matrices de taille 1000x1000
set.seed(123)
A <- matrix(rnorm(1000^2), nrow = 1000, ncol = 1000)
B <- matrix(rnorm(1000^2), nrow = 1000, ncol = 1000)

# Liste des sections et cores à tester
liste_coeurs <- c(1, 2, 3, 4, 5, 6, 8)
resultats_multiplication <- data.frame(Cœurs = liste_coeurs, Temps = NA, Speedup = NA)

# Calcul séquentiel de référence (temps pour la multiplication sans parallélisme)
temps_seq <- system.time({
  res_seq <- multiplication_matrice(A, B, 1:nrow(A), 1:ncol(B))
})[3]

# Remplissage du tableau des résultats
for (i in seq_along(liste_coeurs)) {
  nb_coeurs <- liste_coeurs[i]
  
  if (nb_coeurs == 1) {
    resultats_multiplication$Temps[i] <- round(temps_seq, 4)
    resultats_multiplication$Speedup[i] <- 1
  } else {
    temps_par <- system.time(calcul_multiplication_parallele(A, B, sections = nb_coeurs))[3]
    resultats_multiplication$Temps[i] <- round(temps_par, 4)
    resultats_multiplication$Speedup[i] <- round(temps_seq / temps_par, 2)
  }
}

# Affichage uniquement du tableau récapitulatif des résultats
print(resultats_multiplication)
```

### Efficacité

L'efficacité d'un calcul parallèle peut être définie comme le rapport entre le temps d'exécution séquentiel et le temps d'exécution parallèle, normalisé par le nombre de cœurs utilisés, et donc c'est égale au rapport du speedup sur le nombre de coeurs: 

\[
E = \frac{S}{N_{\text{coeurs}}} = \frac{T_{\text{par}} \times N_{\text{coeurs}}}{T_{\text{seq}}}
\]

En d'autres termes, l'efficacité mesure combien de travail parallèle chaque cœur accomplit. Plus l'efficacité est élevée, plus le calcul est optimal avec un nombre donné de cœurs. 

Tracons la courbe d'efficacité avec les valeurs obtenues precedemment. 

```{r efficace, message=FALSE, warning=FALSE}
# Données des résultats
cœurs <- c(1, 2, 3, 4, 5, 6, 8)
speedup <- c(1.00, 2.12, 2.47, 2.91, 3.15, 3.38, 3.61)

# Calcul de l'efficacité
efficacité <- speedup / cœurs

# Tracer la courbe d'efficacité
plot(cœurs, efficacité, type = "b", col = "blue", pch = 16, xlab = "Nombre de cœurs", ylab = "Efficacité", 
     main = "Courbe d'efficacité en fonction du nombre de cœurs")
grid()
```

On observe que l’efficacité diminue progressivement à mesure qu’on ajoute des cœurs.
Cela s’explique par le temps supplémentaire consacré à la coordination :
création des clusters, répartition des données, synchronisation des résultats… Ces opérations, bien que nécessaires, n'accélèrent pas directement le calcul.

Ainsi, au-delà d’un certain nombre de cœurs, l’ajout de ressources ne se traduit plus par un gain proportionnel de performance.

### Scalabilité

La scalabilité est étroitement liée à l’efficacité, mais elle s’intéresse à l’évolution de la performance lorsqu’on augmente les ressources.

Elle mesure la capacité d’un programme à maintenir ou améliorer ses performances lorsqu’on augmente le nombre de cœurs, la taille des données ou la mémoire disponible.

- Si un programme est **hautement scalable**, le temps d’exécution diminue régulièrement à mesure qu’on ajoute des cœurs.
- Si la scalabilité est **faible**, ajouter des ressources ne sert plus à rien au bout d’un certain point — voire ça ralentit à cause de la coordination nécessaire entre les threads (synchronisation, accès partagés, etc.).

## Défis et optimisation

Différents défis affectent l'efficacité de la parallélisation. 

### a. Overhead

L'**overhead** (ou surcharge) fait référence à la quantité de travail supplémentaire introduite lors de l'utilisation du parallélisme. Bien que l'idée soit de répartir le travail sur plusieurs cœurs pour accélérer l'exécution, il y a souvent des coûts associés à la gestion de cette parallélisation.

#### Exemples d'overhead :

- **Diviser le travail** : Lorsque le travail est partagé entre les cœurs, une certaine quantité de temps est nécessaire pour diviser les tâches et les distribuer.
- **Gestion de la mémoire** : Il faut souvent allouer et gérer de la mémoire pour chaque cœur, ce qui peut entraîner une surcharge supplémentaire.
- **Communication entre les cœurs** : Si les différents cœurs doivent partager des informations ou combiner leurs résultats, cela peut ralentir le processus global, car les cœurs doivent attendre pour s'échanger ces données. 

#### Comment l'optimiser ?

- **Optimiser la taille des tâches** :  Si une tâche est trop petite, la gestion de cette petite tâche prend plus de temps que l'exécution elle-même, ce qui augmente l'overhead. Il faut trouver un bon équilibre dans la taille des tâches.

- **Utilisation d'algorithmes parallèles efficaces** : Choisir des algorithmes conçus pour minimiser l'overhead, comme ceux qui nécessitent moins de communication entre les cœurs.

### b. Problèmes de Synchronisation

Les **problèmes de synchronisation** surviennent lorsqu'il y a des dépendances entre les tâches qui nécessitent que certains processus attendent l'achèvement des autres avant de pouvoir poursuivre. Ce phénomène peut entraîner des retards importants et une perte d'efficacité dans un système parallèle.

#### Exemples de problèmes de synchronisation :

- **Attente des résultats** : Si une tâche dépend des résultats d'une autre tâche, les cœurs doivent attendre la fin de la tâche précédente avant de continuer, ce qui peut ralentir l'ensemble du processus.
- **Deadlock (verrouillage mutuel)** : Un état où deux ou plusieurs tâches sont en attente indéfinie les unes des autres, ce qui arrête complètement le programme.
- **Race conditions (conditions de concurrence)** : Des erreurs peuvent se produire si plusieurs cœurs essaient de modifier la même donnée en même temps sans coordination, ce qui peut entraîner des résultats incorrects.

#### Comment l'optimiser ?

- **Utiliser des mécanismes de synchronisation efficaces**  pour contrôler l'accès aux ressources partagées.
- **Réduire les dépendances** : Concevoir les algorithmes de manière à minimiser les tâches dépendantes les unes des autres, de sorte que les cœurs puissent fonctionner de manière plus indépendante.
- **Optimiser la granularité des tâches** : Parfois, diviser les tâches en morceaux plus petits et indépendants permet de réduire les attentes liées à la synchronisation.

### c. Stratégies : Taille des Tâches, Choix du Nombre de Workers

Les stratégies d'optimisation se concentrent sur deux éléments clés : la **taille des tâches** et le **choix du nombre de workers** (cœurs ou threads utilisés).

#### Taille des Tâches

- **Trop petite** : Une tâche trop petite peut entraîner un overhead élevé à cause de la gestion de la tâche et de la synchronisation.
- **Trop grande** : Si une tâche est trop volumineuse et est attribuée à un seul cœur ou à un nombre limité de cœurs, les autres cœurs qui pourraient être utilisés pour traiter différentes parties du travail restent inactifs ou sous-utilisés. Cela entraîne des périodes d'attente inutiles pour ces cœurs, qui attendent que le cœur principal termine sa portion de travail. Ce déséquilibre dans l'utilisation des ressources réduit l'efficacité globale du système.

**Optimisation** : Il est important de trouver une taille de tâche optimale qui permet à chaque cœur de travailler efficacement tout en minimisant l'overhead de gestion.

#### Choix du Nombre de Workers (Cœurs/Threads)

- **Trop de workers** : Trop de workers peuvent entraîner une concurrence excessive pour les ressources, un conflit de cache et un overhead de communication.
- **Trop peu de workers** : Trop peu de workers peuvent ne pas exploiter pleinement les ressources disponibles.

**Optimisation** : Il est crucial de choisir le nombre de workers en fonction du problème à résoudre et du nombre de cœurs disponibles. Idéalement, le nombre de workers devrait être en accord avec la taille du problème et la capacité de calcul du système.
