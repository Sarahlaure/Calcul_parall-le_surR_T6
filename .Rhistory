clusterExport(cl, c("tri_fusion_par", "fusion"))
return(cl)
}
# Exécution
set.seed(123)
donnees <- sample(1:100000, 50000) # 50 000 éléments
cl <- initialiser_cluster()
resultat <- tri_fusion_par(donnees, cl)
# Tâche principale : additionner quatre vecteurs de 1e6 nombres
vec_list <- list(rnorm(1e6), rnorm(1e6), rnorm(1e6), rnorm(1e6))
# Fork-Join
n_cores <- 4
cl <- makeCluster(n_cores)
# Fork : on distribue chaque vecteur à un cœur
results <- parLapply(cl, vec_list, function(v) sum(v))
# Join : on additionne les sommes partielles
total_sum <- Reduce("+", results)
stopCluster(cl)
total_sum
library(parallel)
# Tri fusion parallèle en français
tri_fusion_parallele <- function(liste) {
n <- length(liste)
if (n <= 1) return(liste)    # Base : liste d’un seul élément
milieu <- floor(n / 2)
# Créer un cluster de 2 cœurs pour trier les moitiés en parallèle
cl <- makeCluster(2)
clusterExport(cl, varlist = c("tri_fusion_parallele"))
# Diviser et lancer récursion en parallèle (fork)
moities <- parLapply(cl, list(
liste[1:milieu],
liste[(milieu + 1):n]
), tri_fusion_parallele)
stopCluster(cl)
gauche <- moities[[1]]
droite <- moities[[2]]
# Combiner (fusionner deux listes triées)
i <- j <- 1
resultat <- integer(0)
while (i <= length(gauche) && j <= length(droite)) {
if (gauche[i] <= droite[j]) {
resultat <- c(resultat, gauche[i]); i <- i + 1
} else {
resultat <- c(resultat, droite[j]); j <- j + 1
}
}
c(resultat, gauche[i:length(gauche)], droite[j:length(droite)])
}
# Exemple d’utilisation
set.seed(123)
ma_liste <- sample(1:100, 20, replace = TRUE)
cat("Liste non triée :", ma_liste, "\n\n")
liste_triee <- tri_fusion_parallele(ma_liste)
library(parallel)
# Tri fusion parallèle en français
tri_fusion_parallele <- function(liste) {
n <- length(liste)
if (n <= 1) return(liste)
milieu <- floor(n / 2)
cl <- makeCluster(2)
# Charger le package et exporter la fonction dans chaque nœud
clusterEvalQ(cl, library(parallel))  # <<-- ligne ajoutée ici
clusterExport(cl, varlist = c("tri_fusion_parallele"))
# Appel récursif parallèle
moities <- parLapply(cl, list(
liste[1:milieu],
liste[(milieu + 1):n]
), tri_fusion_parallele)
stopCluster(cl)
gauche <- moities[[1]]
droite <- moities[[2]]
# Fusion des deux listes triées
i <- j <- 1
resultat <- integer(0)
while (i <= length(gauche) && j <= length(droite)) {
if (gauche[i] <= droite[j]) {
resultat <- c(resultat, gauche[i]); i <- i + 1
} else {
resultat <- c(resultat, droite[j]); j <- j + 1
}
}
c(resultat, gauche[i:length(gauche)], droite[j:length(droite)])
}
# Exemple
set.seed(123)
ma_liste <- sample(1:100, 20, replace = TRUE)
cat("Liste non triée :", ma_liste, "\n\n")
liste_triee <- tri_fusion_parallele(ma_liste)
library(parallel)
# Tri fusion parallèle en français
tri_fusion_parallele <- function(liste) {
n <- length(liste)
if (n <= 1) return(liste)
milieu <- floor(n / 2)
cl <- makeCluster(2)
# Charger le package et exporter la fonction dans chaque nœud
clusterEvalQ(cl, library(parallel))  # <<-- ligne ajoutée ici
clusterExport(cl, varlist = c("tri_fusion_parallele"))
# Appel récursif parallèle
moities <- parLapply(cl, list(
liste[1:milieu],
liste[(milieu + 1):n]
), tri_fusion_parallele)
stopCluster(cl)
gauche <- moities[[1]]
droite <- moities[[2]]
# Fusion des deux listes triées
i <- j <- 1
resultat <- integer(0)
while (i <= length(gauche) && j <= length(droite)) {
if (gauche[i] <= droite[j]) {
resultat <- c(resultat, gauche[i]); i <- i + 1
} else {
resultat <- c(resultat, droite[j]); j <- j + 1
}
}
c(resultat, gauche[i:length(gauche)], droite[j:length(droite)])
}
# Exemple
set.seed(123)
ma_liste <- sample(1:100, 20, replace = TRUE)
cat("Liste non triée :", ma_liste, "\n\n")
liste_triee <- tri_fusion_parallele(ma_liste)
setwd("E:/ISEP 2/MON DOSSIER/APPRENTISSAGE DES VACANCES/ISEP3/ME _ SEMESTRE 2/PROJET STATISTIQUE SOUS R ET PYTHON/GITHUB EXPOSE/Calcul_parall-le_surR_T6")
# Texte à analyser (exemple simple)
phrases <- c(
"Le chat mange le poisson",
"Le chien court après le chat",
"Le poisson nage dans l'eau"
)
# --- Phase MAP ---
map_function <- function(phrase) {
# Découpage de la phrase en mots (clés)
mots <- unlist(strsplit(tolower(phrase), "\\W+"))  # Ignorer la casse et la ponctuation
# Génération des couples (mot, 1)
return(data.frame(mot = mots, valeur = 1))
}
# --- Phase REDUCE ---
reduce_function <- function(cles, valeurs) {
# Agrégation : somme des valeurs par clé
aggregate(valeur ~ mot, data = data.frame(mot = cles, valeur = valeurs), sum)
}
# Configuration du cluster (4 cœurs)
cl <- makeCluster(4)
clusterExport(cl, c("map_function", "reduce_function"))
# --- MAP ---
# Appliquer la fonction map à chaque phrase en parallèle
resultats_map <- parLapply(cl, phrases, map_function)
# --- SHUFFLE (Regroupement des clés) ---
# Combiner tous les résultats partiels
donnees <- do.call(rbind, resultats_map)
# --- REDUCE ---
# Appliquer la fonction reduce sur les données groupées
resultat_final <- reduce_function(donnees$mot, donnees$valeur)
# Arrêt du cluster
stopCluster(cl)
print(resultat_final)
# Texte à analyser (exemple simple)
phrases <- c(
"Le chat mange le poisson",
"Le chien court après le chat",
"Le poisson nage dans l'eau"
)
# --- Phase MAP ---
map_function <- function(phrase) {
# Découpage de la phrase en mots (clés)
mots <- unlist(strsplit(tolower(phrase), "\\W+"))  # Ignorer la casse et la ponctuation
# Génération des couples (mot, 1)
return(data.frame(mot = mots, valeur = 1))
}
# --- Phase REDUCE ---
reduce_function <- function(cles, valeurs) {
# Agrégation : somme des valeurs par clé
aggregate(valeur ~ mot, data = data.frame(mot = cles, valeur = valeurs), sum)
}
# Configuration du cluster (4 cœurs)
cl <- makeCluster(4)
clusterExport(cl, c("map_function", "reduce_function"))
# --- MAP ---
# Appliquer la fonction map à chaque phrase en parallèle
resultats_map <- parLapply(cl, phrases, map_function)
# --- SHUFFLE (Regroupement des clés) ---
# Combiner tous les résultats partiels
donnees <- do.call(rbind, resultats_map)
# --- REDUCE ---
# Appliquer la fonction reduce sur les données groupées
resultat_final <- reduce_function(donnees$mot, donnees$valeur)
# Arrêt du cluster
stopCluster(cl)
print(resultat_final)
# Ce package n'a pas besoin d'être téléchargé au préalable,
# il est directement disponible lorsque R est installé
library(parallel)
# Nombre de coeurs physiques
nb_coeurs_physiques <- detectCores(logical = FALSE)
print(nb_coeurs_physiques)
ships <- read_csv("../data/ships.csv")
# 1. Préparation des données
donnees_filtrees <- ships %>%
filter(!is.na(SPEED), SPEED > 0, !is.na(ship_type))
types_navires <- unique(donnees_filtrees$ship_type)
# Fonction de calcul de la moyenne pour un type de navire
calculer_moyenne <- function(type) {
donnees_filtrees %>%
filter(ship_type == type) %>%
summarise(type_navire = type,
vitesse_moyenne = mean(SPEED, na.rm = TRUE))
}
# 2. Fonction de calcul parallèle avec 8 workers
parallele <- function() {
# Créer le cluster avec 8 workers
cl <- makeCluster(8)
# Exporter les objets nécessaires aux workers
clusterExport(cl, varlist = c("donnees_filtrees", "types_navires", "calculer_moyenne"), envir = globalenv())
# Charger dplyr dans chaque worker
clusterEvalQ(cl, library(dplyr))
# Mesurer le temps de calcul
temps_calc <- system.time({
res <- parLapply(cl, types_navires, calculer_moyenne)
})[3]
# Arrêter le cluster
stopCluster(cl)
# Agréger les résultats
resultat_final <- bind_rows(res)
return(list(resultats = resultat_final, temps = temps_calc))
}
# 3. Appel de la fonction et affichage des résultats
resultats <- parallele()
cat("Temps de calcul avec 12 workers :", resultats$temps, "secondes\n")
# Étape 1 : Préparation des données
donnees_filtrees <- ships %>%
filter(!is.na(SPEED), SPEED > 0, !is.na(ship_type))
# Étape 2 : Définition des tâches hétérogènes (chaque tâche est une fonction différente)
tache_moyennes <- function(data) {
data %>%
group_by(ship_type) %>%
summarise(vitesse_moyenne = mean(SPEED, na.rm = TRUE))
}
tache_regression <- function(data) {
lm(SPEED ~ LENGTH, data = data)  # Modèle linéaire
}
tache_statistiques <- function(data) {
data %>%
group_by(ship_type) %>%
summarise(
mediane = median(SPEED, na.rm = TRUE),
ecart_type = sd(SPEED, na.rm = TRUE)
)
}
# Étape 3 : Configuration du cluster pour 3 workers (un par tâche)
cl <- makeCluster(3)
# Exportation des données et packages vers chaque worker (une seule fois)
clusterExport(cl, c("donnees_filtrees", "tache_moyennes", "tache_regression", "tache_statistiques"))
clusterEvalQ(cl, library(dplyr))
# Étape 4 : Attribution des tâches aux workers avec chronométrage (tictoc)
tic("Exécution des tâches en parallèle")  # Démarrer le chronomètre global
resultats <- parLapply(cl, list(tache_moyennes, tache_regression, tache_statistiques), function(f) {
f(donnees_filtrees)  # Chaque worker exécute une fonction différente
})
toc()
# Étape 5 : Arrêt du cluster et extraction des résultats
stopCluster(cl)
# Extraire les résultats
resultats_moyennes <- resultats[[1]]  # Vitesses moyennes
resultats_regression <- resultats[[2]]    # Objet de régression
resultats_stats <- resultats[[3]]     # Statistiques
# Affichage des résultats
cat("Résultats des vitesses moyennes :\n")
print(resultats_moyennes)
cat("Résumé du modèle de régression :\n")
summary(resultats_regression)
cat("Statistiques :\n")
print(resultats_stats)
# Étape 1 : Préparation des données
donnees_filtrees <- ships %>%
filter(!is.na(SPEED), SPEED > 0, !is.na(ship_type))
# Étape 2 : Définition des tâches hétérogènes (chaque tâche est une fonction différente)
tache_moyennes <- function(data) {
data %>%
group_by(ship_type) %>%
summarise(vitesse_moyenne = mean(SPEED, na.rm = TRUE))
}
tache_regression <- function(data) {
lm(SPEED ~ LENGTH, data = data)  # Modèle linéaire
}
tache_statistiques <- function(data) {
data %>%
group_by(ship_type) %>%
summarise(
mediane = median(SPEED, na.rm = TRUE),
ecart_type = sd(SPEED, na.rm = TRUE)
)
}
# Étape 3 : Configuration du cluster pour 3 workers (un par tâche)
cl <- makeCluster(3)
# Exportation des données et packages vers chaque worker (une seule fois)
clusterExport(cl, c("donnees_filtrees", "tache_moyennes", "tache_regression", "tache_statistiques"))
clusterEvalQ(cl, library(dplyr))
# Étape 4 : Attribution des tâches aux workers avec chronométrage (tictoc)
tic("Exécution des tâches en parallèle")  # Démarrer le chronomètre global
resultats <- parLapply(cl, list(tache_moyennes, tache_regression, tache_statistiques), function(f) {
f(donnees_filtrees)  # Chaque worker exécute une fonction différente
})
toc()
# Étape 5 : Arrêt du cluster et extraction des résultats
stopCluster(cl)
# Extraire les résultats
resultats_moyennes <- resultats[[1]]  # Vitesses moyennes
resultats_regression <- resultats[[2]]    # Objet de régression
resultats_stats <- resultats[[3]]     # Statistiques
# Fusionner les résultats dans un seul tableau
tableau_resultats <- resultats_moyennes %>%
left_join(resultats_stats, by = "ship_type") %>%
mutate(coef_intercept = resultats_regression[1],  # Ajouter le coefficient d'interception de la régression
coef_slope = resultats_regression[2])  # Ajouter le coefficient de pente de la régression
# Affichage du tableau final
cat("Tableau récapitulatif des résultats :\n")
print(tableau_resultats)
library(doParallel)
registerDoParallel(4)  # Utiliser 4 workers
resultats <- foreach(i = 1:5) %dopar% {
sqrt(i)  # Calcul de la racine carrée de chaque i
}
stopImplicitCluster()
library(doParallel)
registerDoParallel(4)  # Utiliser 4 workers
resultats <- foreach(i = 1:5) %dopar% {
sqrt(i)  # Calcul de la racine carrée de chaque i
}
stopImplicitCluster()
print(resultats)
library(foreach)
library(doParallel)
library(tictoc)
# Enregistrer 4 workers (coeurs)
registerDoParallel(4)
# 💡 Test 1 : version séquentielle (pas en parallèle)
tic("Séquentiel")
result_sequentiel <- foreach(i = 1:5) %do% {
Sys.sleep(1)  # Simule un calcul de 1 seconde
i^2           # Exemple : carré de i
}
toc()
# 💡 Test 2 : version parallèle
tic("Parallèle")
result_parallele <- foreach(i = 1:5) %dopar% {
Sys.sleep(1)  # Même tâche de 1 seconde
i^2
}
toc()
# Arrêter les workers
stopImplicitCluster()
# Affichage des résultats
print(result_sequentiel)
print(result_parallele)
library(foreach)
library(doParallel)
library(tictoc)
# Enregistrer 4 workers (coeurs)
registerDoParallel(4)
# 💡 Test 1 : version séquentielle (pas en parallèle)
tic("Séquentiel")
result_sequentiel <- foreach(i = 1:5) %do% {
i^2           #
}
toc()
# 💡 Test 2 : version parallèle
tic("Parallèle")
result_parallele <- foreach(i = 1:5) %dopar% {
i^2
}
toc()
# Arrêter les workers
stopImplicitCluster()
# Affichage des résultats
print(result_sequentiel)
print(result_parallele)
library(foreach)
library(doParallel)
library(tictoc)
# Enregistrer 4 workers (coeurs)
registerDoParallel(8)
# 💡 Test 1 : version séquentielle (pas en parallèle)
tic("Séquentiel")
result_sequentiel <- foreach(i = 1:5) %do% {
i^2           #
}
toc()
# 💡 Test 2 : version parallèle
tic("Parallèle")
result_parallele <- foreach(i = 1:5) %dopar% {
i^2
}
toc()
# Arrêter les workers
stopImplicitCluster()
# Affichage des résultats
print(result_sequentiel)
print(result_parallele)
library(foreach)
library(doParallel)
library(tictoc)
# Enregistrer 4 workers (coeurs)
registerDoParallel(5)
# 💡 Test 1 : version séquentielle (pas en parallèle)
tic("Séquentiel")
result_sequentiel <- foreach(i = 1:5) %do% {
i^2           #
}
toc()
# 💡 Test 2 : version parallèle
tic("Parallèle")
result_parallele <- foreach(i = 1:5) %dopar% {
i^2
}
toc()
# Arrêter les workers
stopImplicitCluster()
# Affichage des résultats
print(result_sequentiel)
print(result_parallele)
library(doParallel)
registerDoParallel(4)  # Utiliser 4 workers
resultats <- foreach(i = 1:5) %dopar% {
sqrt(i)  # Calcul de la racine carrée de chaque i
}
stopImplicitCluster()
print(resultats)
library(doParallel)
cl <- makeCluster(4)
registerDoParallel(cl)
result <- foreach(i = 1:100) %dopar% sqrt(i)
stopCluster(cl)
print(resultats)
ships <- read_csv("../data/ships.csv")
# 1. Préparation des données
donnees_filtrees <- ships %>%
filter(!is.na(SPEED), SPEED > 0, !is.na(ship_type))
types_navires <- unique(donnees_filtrees$ship_type)
# Fonction de calcul de la moyenne pour un type de navire
calculer_moyenne <- function(type) {
donnees_filtrees %>%
filter(ship_type == type) %>%
summarise(type_navire = type,
vitesse_moyenne = mean(SPEED, na.rm = TRUE))
}
# 2. Fonction de calcul parallèle avec 8 workers
parallele <- function() {
# Créer le cluster avec 8 workers
cl <- makeCluster(8)
# Exporter les objets nécessaires aux workers
clusterExport(cl, varlist = c("donnees_filtrees", "types_navires", "calculer_moyenne"), envir = globalenv())
# Charger dplyr dans chaque worker
clusterEvalQ(cl, library(dplyr))
# Mesurer le temps de calcul
temps_calc <- system.time({
res <- parLapply(cl, types_navires, calculer_moyenne)
})[3]
# Arrêter le cluster
stopCluster(cl)
# Agréger les résultats
resultat_final <- bind_rows(res)
return(list(resultats = resultat_final, temps = temps_calc))
}
# 3. Appel de la fonction et affichage des résultats
resultats <- parallele()
cat("Temps de calcul avec 8 workers :", resultats$temps, "secondes\n")
# Étape 1 : Préparation des données
donnees_filtrees <- ships %>%
filter(!is.na(SPEED), SPEED > 0, !is.na(ship_type))
# Étape 2 : Définition des tâches hétérogènes (chaque tâche est une fonction différente)
tache_moyennes <- function(data) {
data %>%
group_by(ship_type) %>%
summarise(vitesse_moyenne = mean(SPEED, na.rm = TRUE))
}
tache_regression <- function(data) {
lm(SPEED ~ LENGTH, data = data)  # Modèle linéaire
}
tache_statistiques <- function(data) {
data %>%
group_by(ship_type) %>%
summarise(
mediane = median(SPEED, na.rm = TRUE),
ecart_type = sd(SPEED, na.rm = TRUE)
)
}
# Étape 3 : Configuration du cluster pour 3 workers (un par tâche)
cl <- makeCluster(3)
# Exportation des données et packages vers chaque worker (une seule fois)
clusterExport(cl, c("donnees_filtrees", "tache_moyennes", "tache_regression", "tache_statistiques"))
clusterEvalQ(cl, library(dplyr))
# Étape 4 : Attribution des tâches aux workers avec chronométrage (tictoc)
tic("Exécution des tâches en parallèle")  # Démarrer le chronomètre global
resultats <- parLapply(cl, list(tache_moyennes, tache_regression, tache_statistiques), function(f) {
f(donnees_filtrees)  # Chaque worker exécute une fonction différente
})
toc()
# Étape 5 : Arrêt du cluster et extraction des résultats
stopCluster(cl)
# Extraire les résultats
resultats_moyennes <- resultats[[1]]  # Vitesses moyennes
resultats_regression <- resultats[[2]]    # Objet de régression
resultats_stats <- resultats[[3]]     # Statistiques
# Fusionner les résultats dans un seul tableau
tableau_resultats <- resultats_moyennes %>%
left_join(resultats_stats, by = "ship_type") %>%
mutate(coef_intercept = resultats_regression[1],  # Ajouter le coefficient d'interception de la régression
coef_slope = resultats_regression[2])  # Ajouter le coefficient de pente de la régression
# Affichage du tableau final
cat("Tableau récapitulatif des résultats :\n")
print(tableau_resultats)
