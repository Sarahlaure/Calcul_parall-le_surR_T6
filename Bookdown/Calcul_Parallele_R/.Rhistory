cat("\n-- Résultats Séquentiel --\n")
for (n in names(res_sequential)) {
cat(n, ":\n")
print(res_sequential[[n]])
}
# Chargement des librairies
library(parallel)
library(dplyr)
library(readr)
library(tictoc)
# Chargement des données (silence sur les specs)
ships <- read_csv("Donnees/ships.csv", show_col_types = FALSE)
# Filtrer valeurs de SPEED et LENGTH valides
donnees <- ships %>% filter(!is.na(SPEED), SPEED > 0, !is.na(LENGTH))
# Définition des 4 tâches
#   1) moyenne de SPEED
#   2) écart-type de SPEED
#   3) coefficients de la régression SPEED ~ LENGTH
#   4) quantiles de SPEED (10%,25%,50%,75%,90%)
taches <- list(
moyenne_speed = function(df) {
mean(df$SPEED, na.rm = TRUE)
},
sd_speed = function(df) {
sd(df$SPEED, na.rm = TRUE)
},
regression_speed_length = function(df) {
coef(lm(SPEED ~ LENGTH, data = df))
},
quantiles_speed = function(df) {
quantile(df$SPEED, probs = c(0.10, 0.25, 0.50, 0.75, 0.90), na.rm = TRUE)
}
)
# Nombre de workers = 4
n_workers <- length(taches) #On fait ainsi au cas où modifie le nombre de taches plutard
#  Exécution parallèle
tic.clearlog()
tic("Parallèle")
cl <- makeCluster(n_workers)
clusterExport(cl, c("donnees", "taches"), envir = environment())
invisible(clusterEvalQ(cl, suppressMessages(library(dplyr))))
res_parallel <- parLapply(cl, taches, function(f) f(donnees))
stopCluster(cl)
toc(log = TRUE)
# Chargement des librairies
library(parallel)
library(dplyr)
library(readr)
library(tictoc)
# Chargement des données (silence sur les specs)
ships <- read_csv("Donnees/ships.csv", show_col_types = FALSE)
# Filtrer valeurs de SPEED et LENGTH valides
donnees <- ships %>% filter(!is.na(SPEED), SPEED > 0, !is.na(LENGTH))
# Définition des 4 tâches
#   1) moyenne de SPEED
#   2) écart-type de SPEED
#   3) coefficients de la régression SPEED ~ LENGTH
#   4) quantiles de SPEED (10%,25%,50%,75%,90%)
taches <- list(
moyenne_speed = function(df) {
mean(df$SPEED, na.rm = TRUE)
},
sd_speed = function(df) {
sd(df$SPEED, na.rm = TRUE)
},
regression_speed_length = function(df) {
coef(lm(SPEED ~ LENGTH, data = df))
},
quantiles_speed = function(df) {
quantile(df$SPEED, probs = c(0.10, 0.25, 0.50, 0.75, 0.90), na.rm = TRUE)
}
)
# Nombre de workers = 4
n_workers <- length(taches) #On fait ainsi au cas où modifie le nombre de taches plutard
# 1. Exécution parallèle
tic.clearlog() # Nettoyage des logs de tictoc
tic("Parallèle")
cl <- makeCluster(n_workers)
clusterExport(cl, c("donnees", "taches"), envir = environment())
invisible(clusterEvalQ(cl, suppressMessages(library(dplyr))))
res_parallel <- parLapply(cl, taches, function(f) f(donnees))
stopCluster(cl)
toc(log = TRUE)
# 2. Affichage des résultats
cat("\n-- Résultats Parallèle --\n")
for (n in names(res_parallel)) {
cat(n, ":\n")
print(res_parallel[[n]])
}
library(readr)
ships <- readr::read_csv("Donnees/ships.csv", show_col_types = FALSE) # Pour ne pas afficher les infos sur les colonnes
# Étape 1 : Préparation des données (données sans valeurs manquantes)
donnees_filtrees <- ships %>%
filter(!is.na(SPEED), SPEED > 0)
# Étape 2 : Définition de la fonction
tache_moyenne <- function(data) {
mean(data$SPEED, na.rm = TRUE)
}
# Étape 3 : Configuration du cluster pour 3 workers
cl <- makeCluster(2)
# Exportation des données et fonction vers chaque worker
clusterExport(cl, c("donnees_filtrees", "tache_moyenne"))
invisible(clusterEvalQ(cl, suppressMessages(library(dplyr)))) # Juste pour désactiver les messages pour les librairies
# Étape 4 : Exécution parallèle avec chronométrage
tic("Exécution en parallèle")
resultats_paralleles <- parLapply(cl, list(tache_moyenne), function(f) {
res <- f(donnees_filtrees)  # Chaque worker calcule la moyenne de SPEED
cat("Résultat du cœur :", res, "\n")  # Affiche le résultat de chaque cœur
return(res)
})
toc()  # Chronomètre le temps d'exécution
# Étape 5 : Arrêt du cluster
stopCluster(cl)
# Résultat parallèle
moyenne_parallele <- resultats_paralleles[[1]]
cat("Moyenne (calculée en parallèle) :", moyenne_parallele, "\n")
# Calcul séquentiel pour comparaison
tic("Exécution séquentielle")
moyenne_sequentielle <- mean(donnees_filtrees$SPEED, na.rm = TRUE)
toc()
# Affichage du résultat séquentiel
cat("Moyenne (calculée séquentiellement) :", moyenne_sequentielle, "\n")
library(readr)
ships <- readr::read_csv("Donnees/ships.csv", show_col_types = FALSE) # Pour ne pas afficher les infos sur les colonnes
# Étape 1 : Préparation des données (données sans valeurs manquantes)
donnees_filtrees <- ships %>%
filter(!is.na(SPEED), SPEED > 0)
# Étape 2 : Définition de la fonction
tache_moyenne <- function(data) {
mean(data$SPEED, na.rm = TRUE)
}
# Étape 3 : Configuration du cluster pour 3 workers
cl <- makeCluster(2)
# Exportation des données et fonction vers chaque worker
clusterExport(cl, c("donnees_filtrees", "tache_moyenne"))
invisible(clusterEvalQ(cl, suppressMessages(library(dplyr)))) # Juste pour désactiver les messages pour les librairies
# Étape 4 : Exécution parallèle avec chronométrage
tic("Exécution en parallèle")
resultats_paralleles <- parLapply(cl, list(tache_moyenne), function(f) {
res <- f(donnees_filtrees)  # Chaque worker calcule la moyenne de SPEED
cat("Résultat du cœur :", res, "\n")  # Affiche le résultat de chaque cœur
return(res)
})
toc()  # Chronomètre le temps d'exécution
# Étape 5 : Arrêt du cluster
stopCluster(cl)
# Résultat parallèle
moyenne_parallele <- resultats_paralleles[[1]]
cat("Moyenne (calculée en parallèle) :", moyenne_parallele, "\n")
# Charger les librairies nécessaires
suppressPackageStartupMessages({
library(parallel)
})
# Texte à analyser
phrases <- c(
"Le chat mange le poisson",
"Le chien court après le chat",
"Le poisson nage dans l'eau"
)
# --- Phase MAP ---
map_function <- function(phrase) {
# Découpage de la phrase en mots (clés)
mots <- unlist(strsplit(tolower(phrase), "\\W+"))  # Ignorer la casse et la ponctuation
# Génération des couples (mot, 1)
return(data.frame(mot = mots, valeur = 1))
}
# --- Phase REDUCE ---
reduce_function <- function(cles, valeurs) {
# Agrégation : somme des valeurs par clé
aggregate(valeur ~ mot, data = data.frame(mot = cles, valeur = valeurs), sum)
}
# Configuration du cluster (4 cœurs)
cl <- makeCluster(4)
clusterExport(cl, c("map_function", "reduce_function"))
# --- MAP ---
# Appliquer la fonction map à chaque phrase en parallèle
resultats_map <- parLapply(cl, phrases, map_function)
# --- SHUFFLE (Regroupement des clés) ---
# Combiner tous les résultats partiels
donnees <- do.call(rbind, resultats_map)
# --- REDUCE ---
# Appliquer la fonction reduce sur les données groupées
resultat_final <- reduce_function(donnees$mot, donnees$valeur)
# Arrêt du cluster
stopCluster(cl)
print(resultat_final)
library(readr)
library(dplyr)
ships <- readr::read_csv("Donnees/ships.csv", show_col_types = FALSE) # Pour ne pas afficher les infos sur les colonnes
# Étape 1 : Préparation des données (données sans valeurs manquantes)
donnees_filtrees <- ships %>%
filter(!is.na(SPEED), SPEED > 0)
# Étape 2 : Définition de la fonction
tache_moyenne <- function(data) {
mean(data$SPEED, na.rm = TRUE)
}
# Étape 3 : Configuration du cluster pour 3 workers
cl <- makeCluster(2)
# Exportation des données et fonction vers chaque worker
clusterExport(cl, c("donnees_filtrees", "tache_moyenne"))
invisible(clusterEvalQ(cl, suppressMessages(library(dplyr)))) # Juste pour désactiver les messages pour les librairies
# Étape 4 : Exécution parallèle avec chronométrage
tic("Exécution en parallèle")
resultats_paralleles <- parLapply(cl, list(tache_moyenne), function(f) {
res <- f(donnees_filtrees)  # Chaque worker calcule la moyenne de SPEED
cat("Résultat du cœur :", res, "\n")  # Affiche le résultat de chaque cœur
return(res)
})
toc()  # Chronomètre le temps d'exécution
# Étape 5 : Arrêt du cluster
stopCluster(cl)
# Résultat parallèle
moyenne_parallele <- resultats_paralleles[[1]]
cat("Moyenne (calculée en parallèle) :", moyenne_parallele, "\n")
library(readr)
library(dplyr)
library(parallel)
library(tictoc)
ships <- readr::read_csv("Donnees/ships.csv", show_col_types = FALSE) # Pour ne pas afficher les infos sur les colonnes
# Étape 1 : Préparation des données (données sans valeurs manquantes)
donnees_filtrees <- ships %>%
filter(!is.na(SPEED), SPEED > 0)
# Étape 2 : Définition de la fonction
tache_moyenne <- function(data) {
mean(data$SPEED, na.rm = TRUE)
}
# Étape 3 : Configuration du cluster pour 3 workers
cl <- makeCluster(2)
# Exportation des données et fonction vers chaque worker
clusterExport(cl, c("donnees_filtrees", "tache_moyenne"))
invisible(clusterEvalQ(cl, suppressMessages(library(dplyr)))) # Juste pour désactiver les messages pour les librairies
# Étape 4 : Exécution parallèle avec chronométrage
tic("Exécution en parallèle")
resultats_paralleles <- parLapply(cl, list(tache_moyenne), function(f) {
res <- f(donnees_filtrees)  # Chaque worker calcule la moyenne de SPEED
cat("Résultat du cœur :", res, "\n")  # Affiche le résultat de chaque cœur
return(res)
})
toc()  # Chronomètre le temps d'exécution
# Étape 5 : Arrêt du cluster
stopCluster(cl)
# Résultat parallèle
moyenne_parallele <- resultats_paralleles[[1]]
cat("Moyenne (calculée en parallèle) :", moyenne_parallele, "\n")
library(tictoc)
# Charger les librairies nécessaires
suppressPackageStartupMessages({
library(parallel)
})
# Texte à analyser
phrases <- c(
"Le chat mange le poisson",
"Le chien court après le chat",
"Le poisson nage dans l'eau"
)
# --- Phase MAP ---
map_function <- function(phrase) {
# Découpage de la phrase en mots (clés)
mots <- unlist(strsplit(tolower(phrase), "\\W+"))  # Ignorer la casse et la ponctuation
# Génération des couples (mot, 1)
return(data.frame(mot = mots, valeur = 1))
}
# --- Phase REDUCE ---
reduce_function <- function(cles, valeurs) {
# Agrégation : somme des valeurs par clé
aggregate(valeur ~ mot, data = data.frame(mot = cles, valeur = valeurs), sum)
}
# Configuration du cluster (4 cœurs)
cl <- makeCluster(4)
clusterExport(cl, c("map_function", "reduce_function"))
# --- MAP ---
# Appliquer la fonction map à chaque phrase en parallèle
resultats_map <- parLapply(cl, phrases, map_function)
# --- SHUFFLE (Regroupement des clés) ---
# Combiner tous les résultats partiels
donnees <- do.call(rbind, resultats_map)
# --- REDUCE ---
# Appliquer la fonction reduce sur les données groupées
resultat_final <- reduce_function(donnees$mot, donnees$valeur)
# Arrêt du cluster
stopCluster(cl)
print(resultat_final)
library(readr)
library(dplyr)
library(parallel)
library(tictoc)
# Étape 1 : Charger les données
ships <- readr::read_csv("Donnees/ships.csv", show_col_types = FALSE)
# Étape 2 : Préparation des données (données sans valeurs manquantes)
donnees_filtrees <- ships %>%
filter(!is.na(SPEED), SPEED > 0)
# Étape 3 : Définir la fonction pour calculer la moyenne
tache_moyenne <- function(data) {
mean(data$SPEED, na.rm = TRUE)
}
# Étape 4 : Configuration du cluster pour 2 workers
cl <- makeCluster(2)
# Exporter les données et la fonction vers chaque worker
clusterExport(cl, c("donnees_filtrees", "tache_moyenne"))
invisible(clusterEvalQ(cl, suppressMessages(library(dplyr))))  # Désactiver les messages
# Étape 5 : Diviser les données en 2 morceaux
donnees_divisees <- split(donnees_filtrees, rep(1:2, each = nrow(donnees_filtrees) / 2))
# Étape 6 : Exécution parallèle avec chronométrage
tic("Exécution en parallèle")
resultats_paralleles <- parLapply(cl, donnees_divisees, tache_moyenne)
toc()  # Chronomètre le temps d'exécution
# Étape 7 : Arrêt du cluster
stopCluster(cl)
# Résultat parallèle
moyenne_parallele <- sum(resultats_paralleles)  # Additionne les résultats des workers
cat("Moyenne (calculée en parallèle) :", moyenne_parallele, "\n")
library(readr)
library(dplyr)
library(parallel)
library(tictoc)
ships <- readr::read_csv("Donnees/ships.csv", show_col_types = FALSE) # Pour ne pas afficher les infos sur les colonnes
# Étape 1 : Préparation des données (données sans valeurs manquantes)
donnees_filtrees <- ships %>%
filter(!is.na(SPEED), SPEED > 0)
# Étape 2 : Définition de la fonction
tache_moyenne <- function(data) {
mean(data$SPEED, na.rm = TRUE)
}
# Étape 3 : Configuration du cluster pour 3 workers
cl <- makeCluster(2)
# Exportation des données et fonction vers chaque worker
clusterExport(cl, c("donnees_filtrees", "tache_moyenne"))
invisible(clusterEvalQ(cl, suppressMessages(library(dplyr)))) # Juste pour désactiver les messages pour les librairies
# Étape 4 : Exécution parallèle avec chronométrage
tic("Exécution en parallèle")
resultats_paralleles <- parLapply(cl, list(tache_moyenne), function(f) {
res <- f(donnees_filtrees)  # Chaque worker calcule la moyenne de SPEED
cat("Résultat du cœur :", res, "\n")  # Affiche le résultat de chaque cœur
return(res)
})
toc()  # Chronomètre le temps d'exécution
# Étape 5 : Arrêt du cluster
stopCluster(cl)
# Résultat parallèle
moyenne_parallele <- resultats_paralleles[[1]]
cat("Moyenne (calculée en parallèle) :", moyenne_parallele, "\n")
library(foreach)
library(doParallel)
# Création du cluster pour l'execution des boucles en paralleles
registerDoParallel(2)
resultats <- foreach(i = 1:20) %dopar% sqrt(i)
print(resultats)
library(foreach)
library(doParallel)
# Créer un cluster avec 2 workers
registerDoParallel(2)
# Utiliser foreach pour paralléliser le calcul, en affichant quel worker traite quel nombre
resultats <- foreach(i = 1:20, .combine = c) %dopar% {
# Afficher quel worker traite quel nombre
worker_id <- Sys.getpid()  # Obtenir l'ID du processus du worker
cat("Worker", worker_id, "traite le nombre", i, "\n")  # Afficher l'ID du worker et le nombre qu'il traite
sqrt(i)  # Calculer la racine carrée du nombre
}
# Afficher les résultats
print(resultats)
library(foreach)
library(doParallel)
# Créer un cluster avec 2 workers
registerDoParallel(2)
# Utiliser foreach pour paralléliser le calcul, en affichant quel worker traite quel nombre
resultats <- foreach(i = 1:20, .combine = c) %dopar% {
# Afficher quel worker traite quel nombre
worker_id <- Sys.getpid()  # Obtenir l'ID du processus du worker
cat("Worker", worker_id, "traite le nombre", i, "\n")  # Afficher l'ID du worker et le nombre qu'il traite
sqrt(i)  # Calculer la racine carrée du nombre
}
# Afficher les résultats
print(resultats)
library(foreach)
library(doParallel)
# Création du cluster pour l'execution des boucles en paralleles
registerDoParallel(2)
resultats <- foreach(i = 1:20) %dopar% sqrt(i)
print(resultats)
library(future)
library(future.apply)
# Définir le plan d'exécution parallèle
plan(multisession)  # Utiliser plusieurs cœurs sur tous les systèmes d'exploitation
# Créer une liste de nombres
x <- 1:100
# Appliquer une fonction de manière parallèle
resultats <- future_lapply(x, function(i) sqrt(i))
# Afficher les résultats
print(resultats)
library(future)
library(future.apply)
# Définir le plan d'exécution parallèle
plan(multisession)  # Utiliser plusieurs cœurs sur tous les systèmes d'exploitation
# Créer une liste de nombres
x <- 1:10
# Appliquer une fonction de manière parallèle
resultats <- future_lapply(x, function(i) sqrt(i))
# Afficher les résultats
print(resultats)
library(parallel)
library(tictoc)
# Fonction séquentielle
tache_sequentielle <- function(n = 1e4) {
sum((1:n)^2)
}
# Fonction parallèle
tache_parallele <- function(n = 1e4, coeurs = 2) {
cl <- makeCluster(coeurs)  # Création d'un cluster de travail avec un nombre spécifié de cœurs
clusterExport(cl, varlist = "n")  # Exporter la variable 'n' dans les processus du cluster
resultat <- parLapply(cl, split(1:n, cut(1:n, coeurs)), function(x) sum(x^2))  # Calcul parallèle des carrés, chaque sous-ensemble 'x' est traité sur un cœur
stopCluster(cl)  # Arrêter le cluster une fois le calcul terminé
Reduce("+", resultat)  # Agréger les résultats obtenus sur les différents cœurs
}
# Liste des cœurs à tester
liste_coeurs <- c(1, 2, 3, 4, 5, 6, 8)
# Initialisation du tableau des résultats
resultats_simples <- data.frame(Cœurs = liste_coeurs, Temps = NA, Speedup = NA)
# Temps de référence (séquentiel)
temps_seq <- system.time(tache_sequentielle())[3]
# Remplissage du tableau
for (i in seq_along(liste_coeurs)) {
nb_coeurs <- liste_coeurs[i]
if (nb_coeurs == 1) {
resultats_simples$Temps[i] <- round(temps_seq, 4)
resultats_simples$Speedup[i] <- 1
} else {
temps_par <- system.time(tache_parallele(1e4, nb_coeurs))[3]
resultats_simples$Temps[i] <- round(temps_par, 4)
resultats_simples$Speedup[i] <- round(temps_seq / temps_par, 2)
}
}
# Affichage du tableau final
resultats_simples
library(parallel)
library(tictoc)
# Fonction pour multiplier une sous-matrice (partie d'une matrice)
multiplication_matrice <- function(A, B, lignes, colonnes) {
res <- matrix(0, nrow = length(lignes), ncol = length(colonnes))
for (i in seq_along(lignes)) {
for (j in seq_along(colonnes)) {
res[i, j] <- sum(A[lignes[i], ] * B[, colonnes[j]])  # Produit scalaire des lignes et colonnes
}
}
return(res)
}
# Fonction principale pour multiplier deux grandes matrices en parallèle
calcul_multiplication_parallele <- function(A, B, sections = 4) {
n <- nrow(A)
# Diviser les lignes et colonnes en sous-ensembles équilibrés
lignes_par_section <- split(1:n, cut(1:n, sections, labels = FALSE))
colonnes_par_section <- split(1:n, cut(1:n, sections, labels = FALSE))
# Créer un cluster
cl <- makeCluster(detectCores())  # Utilise tous les cœurs disponibles
clusterExport(cl, varlist = c("A", "B", "multiplication_matrice"))  # Exporte seulement les variables nécessaires
# Paralléliser la multiplication
resultats <- parLapply(cl, 1:sections, function(i) {
lignes <- lignes_par_section[[i]]
colonnes <- colonnes_par_section[[i]]
multiplication_matrice(A, B, lignes, colonnes)
})
stopCluster(cl)
# Combiner les résultats dans la matrice finale
matrice_resultat <- matrix(0, nrow = n, ncol = n)
for (i in 1:sections) {
lignes <- lignes_par_section[[i]]
colonnes <- colonnes_par_section[[i]]
matrice_resultat[lignes, colonnes] <- resultats[[i]]
}
return(matrice_resultat)
}
# Créer deux grandes matrices de taille 1000x1000
set.seed(123)
A <- matrix(rnorm(1000^2), nrow = 1000, ncol = 1000)
B <- matrix(rnorm(1000^2), nrow = 1000, ncol = 1000)
# Liste des sections et cores à tester
liste_coeurs <- c(1, 2, 3, 4, 5, 6, 8)
resultats_multiplication <- data.frame(Cœurs = liste_coeurs, Temps = NA, Speedup = NA)
# Calcul séquentiel de référence (temps pour la multiplication sans parallélisme)
temps_seq <- system.time({
res_seq <- multiplication_matrice(A, B, 1:nrow(A), 1:ncol(B))
})[3]
# Remplissage du tableau des résultats
for (i in seq_along(liste_coeurs)) {
nb_coeurs <- liste_coeurs[i]
if (nb_coeurs == 1) {
resultats_multiplication$Temps[i] <- round(temps_seq, 4)
resultats_multiplication$Speedup[i] <- 1
} else {
temps_par <- system.time(calcul_multiplication_parallele(A, B, sections = nb_coeurs))[3]
resultats_multiplication$Temps[i] <- round(temps_par, 4)
resultats_multiplication$Speedup[i] <- round(temps_seq / temps_par, 2)
}
}
# Affichage uniquement du tableau récapitulatif des résultats
print(resultats_multiplication)
# Données des résultats
cœurs <- c(1, 2, 3, 4, 5, 6, 8)
speedup <- c(1.00, 2.12, 2.47, 2.91, 3.15, 3.38, 3.61)
# Calcul de l'efficacité
efficacité <- speedup / cœurs
# Tracer la courbe d'efficacité
plot(cœurs, efficacité, type = "b", col = "blue", pch = 16, xlab = "Nombre de cœurs", ylab = "Efficacité",
main = "Courbe d'efficacité en fonction du nombre de cœurs")
# Données des résultats
cœurs <- c(1, 2, 3, 4, 5, 6, 8)
speedup <- c(1.00, 2.12, 2.47, 2.91, 3.15, 3.38, 3.61)
# Calcul de l'efficacité
efficacité <- speedup / cœurs
# Tracer la courbe d'efficacité
plot(cœurs, efficacité, type = "b", col = "blue", pch = 16, xlab = "Nombre de cœurs", ylab = "Efficacité",
main = "Courbe d'efficacité en fonction du nombre de cœurs")
bookdown::render_book("Page_de_garde.Rmd", "bookdown::gitbook")
